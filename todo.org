Things to do (+ bullet means "done"):
+ Implement the class hierarchy refactoring that Taylor suggested
+ See whether Taylor's View proposal leads to a Crosscat type I believe
+ Fill in easy undefined bits for a full col_step
+ Fill in sampling a view from the view prior 
+ Draft a full column Gibbs sweep
+ Draft row transitions
+ Draft a full row Gibbs sweep
+ Benefit: compile full inference except for hyperparameters
+ Draft initialization against a dataset of the desired type
+ Benefit: Smoke test running the program
  - To get stack traces
    cabal configure --enable-executable-profiling
    cabal build && dist/build/test-smoke/test-smoke +RTS -xc
+ Benefit: benchmark inference vs crosscat on ridiculous data
+ "Crash" testing:
  + Fix the random seed in HaxcatTest and compare to known string
  + Convert TestSmoke.hs into a cabalized benchmark executable
  + Write code checking structural invariants
  + Exercise prediction and assessment (e.g., it executes)
+ "Crash" fixes:
  + Flush empty clusters
+ Refactorings:
  + Flush NoStat?
  + Can I get rid of zipWithHoles via mergeWithKey?
  + Define more compact Show instances for newtypes of ints: Ro, Co, Cl, Vw
  + Taylor says "For NIGNormal pdf, I think you can write this: pdf m
    x = pdf_predictive empty m x.  In fact, that should just be called
    compound_pdf"
  + Introduce a new object, e.g. CRPMixture, which just knows the CRP,
    counts, and partition, and split View into a CRPMixture and the
    current column map.
    + Then Crosscat becomes a CRPMixture and the cc_views thing
+ Elaboration: I want to have partially incorporated views: ones whose
  partition is present and incorporated, but whose component suff
  stats are not.  Prediction is well-defined in this case: use the
  existing cluster assignment, make up an end value, and incorporate
  it into the suffstats.  The trouble is detecting this state; for
  which I seem to want to give views read access to the set of actual
  data items that have been incorporated (as a Map RowID Row,
  presumably)
+ Plan of attack:
  + Pull out CRPMixture or something from View, leaving that and the
    column map
    + define crp_mixture_{{re,un}inc,sample} etc
  + Derive Eq and make the bogo test structural
  + See whether I can pull the same thing from Crosscat
  + Split "view unincorporation" into removing the datum (leaving a
    "crp mixture predict") and removing the latent also (which is what
    the current thing does)
    + Names: view_{cluster,col,row,row_only}_{re,un}inc
  + Define "cc row unincorporation", with the same split
  + Define _predict for CRPMixture, view, and crosscat-rowwise
    + Names: view{_cluster,_row,}_predict
    - The assumption-free version of view predict would need to read
      the dataset in order to repeat previously predicted items (but,
      interestingly, not to incorporate)
  + Should be able to make cc_empty with a row id list and a column id
    list, which can form the basis of a full-size Geweke test
+ Need to incorporate and unincorporate generated data
+ Benefit: geweke test n x m
  + geweke runner ought to have type
    RVar a -> (a -> RVar b) -> (b -> a -> RVar a) -> (a -> b -> Probe) -> Results
    except somebody needs to deal with unincorporating and
    reincorporating any collapsed models in a that depend on the data b
    - Can I make the transition operator do that?
  + Problems with chi2 testing:
    x the test in the statistics package doesn't return the p value
      (wtf?), so I probably need to roll my own
    + the random-fu package has no test, and also has no room for
      survivor functions rather than cdfs in its typeclasses (?)
      - could do it with the plain cdf, I suppose :(
+ Draft predictive simulation
+ Draft predictive pdf
+ Draft multi-row joint simulation
+ Path 1:
  + Define a Plotting.Stub module
  + Write a function there that shells out to Seaborn to draw a heatmap
  + Draw a dependence probability heatmap for 1 cc on synthetic data
  + Generalize to several ccs
- Path 2:
  - Write a Chart program that draws the plot(s) I want to see for
    Chi^2 evidence
  - Use it to (unit) test my Chi^2 method
  - [Option] think about a statistical (probabilistic) unit testing
    framework that displays evidence plots selectively
- Benefit: benchmark inference vs crosscat on synthetic data n x m
- Draft collecting a data table
  + Q: Frames data loading is kooky: it guesses the data's types by
    reading the dataset _at compile time_.
  + A: I guess that's not too bad; and can presumably be overridden at need
  + Q: Frames also depends on base >= 4.7 && < 4.9
    - Could upgrade ghc globally
    - Could learn "stack" and install a local one
  + A: Use stack
  - Version 1:
    - Get a read of Satellites as Doubles only, with missing and NaN
      values parsed out
      + I can't seem to be able to declare my own representation types
        for the columns
      - Storing Maybes, option 1:
        - Define a newtype that means "Maybe Double represented with a sentinel nan"
        - Define a frame parser instance for it using Just . non sentinel
        - Define a VectorFor instance for it
        - Do I need to define an unboxed vector instance for it?
    - The above will be annoying anyway, because I will need to change
      everything to Maybe Double to deal with missing data
- Candidate plan for dynamically-typed crosscat:
  - Parameterize the column store (and everything) by the cell type
  - Define a combinator on Models that accepts an Iso a b and converts
    a Model on a into a Model on b
    - Option: an Iso would have to error out on inappropriate
      arguments; could do something like a Prism that automatically
      ignores them
  - Define a sum combinator on models:
      (Model m1 a, Model m2 a) => Model (Either m1 m2) a
    or even
      (Model m1 a, Model m2 b) => Model (Either m1 m2) (Either a b)
    (which is the composition of the above two)
  - Define the initial universe to be Either Double Int
  - Initialization requires a term that gives the models to use for
    all the columns in the dataset; being a term, it can be computed
    smoothly by a client
  - Pro: Log time access b/c of Map (could move to HashMap)
  - Pro: Single compiled program works on all datasets
  - Pro/Con: Easy to implement, so I am not learning new technology
  - Con: No type safety
  - Con: Default relatively inefficient data representation, but
    fixable
- Can reduce the relative inefficiency of data representation by
  making the universe a newtype that packs Either Double Int into a
  Word64, either in the nan space or lossily with a tag bit
  - Though, the in-model suff stat computations should dominate anyway
    (?)
  - Or by representing Int as a Double that happens to be an integer,
    and permitting the ambiguity
  - This is what Lovcat does for a data representation
- Alternative plan for dynamically-typed crosscat:
  - Can I make the Either column-level, and at least guarantee
    model-data agreement within a column?  Would I have to store
    (pointers to) the column data with the columns for that?
- Candidate plan for statically-typed crosscat:
  - Idea: introduce a type-level list of the data and model types;
    replace column ids by labeled indexing into it
  - Details:
    - Parameterize Column by its hypers and stats types
    - Parameterize View by the list of all possible column types
    - Make the view_columns field a Rec Maybe columns
    - Parameterize Row by all possible column element types (?) and
      make it represented as a Rec Maybe
    - Parameterize Crosscat by the list of all possible column types
      - The views are all the same type, so the map remains OK
    - Parameterize the dataset by the list of column types (like a
      Frame, except I think I want to allow missing data with Maybe)
    - view row {un,re} incorporation will need to zip/map Rec Maybe
      values ignoring the Nothings.
    - view_weights will need to zip and fold Rec Maybe values,
      skipping the Nothings (presumably fold will require first
      mapping to a homogeneous record; I can do that)
    - I need some way to treat the identities of the data columns
      homogeneously, in order to CRP them to view IDs
      - Can probably just keep a constant Record of ColID objects,
        which then map through the cc_partition to ViewIDs.
      - col_sweep becomes weird: I need to be able to operate on a
        zipper over the type-level list of columns, with the place
        being represented, for example, by the ColID and a generic
        lens into the kinds of records I will encounter.
      - Alternately, could define an object which is a column-wise
        zipper on the whole crosscat state (the partition, the done
        portions of all the views, the non-done portions of all the
        views) and an operation that takes the top column off the work
        queue, assigns it, and puts it in the done queue.
        - Has the advantage of not indexing deep into the view
          records, but that cost should be dwarfed by the cost of
          computing column_full_pdf anyway.
    - Will I need to copy all the data in column_major_to_row_major or
      can I get away with allocating the access function?
      - Frame just allocates the access function; the "row major"
        representation is a length and a row-wise indexing function.
  - Pro: Type safety: All the models are guaranteed to be appropriate
    for their respective data.
  - Pro: Efficient data representation
  - Con/Pro: Tricky to implement, so I would be learning new technology
  - Con: Potentially linear-time indexing, at least before vinyl 0.5
    https://github.com/VinylRecords/Vinyl/issues/61
    - Not likely to be a very big deal, because the niglognorm
      computations should dominate runtime
    - Might be fixable by further magic in vinyl
  - Con: Need to compile a new client program for each dataset
- An option is to do both (in separate nested modules) and compare them.
  - To the extent that they manage to share any code, some of the type
    safety checks will carry over
- Benefit: benchmark inference vs crosscat on real data
- Benefit: benchmark simulation vs crosscat on real data
- Benefit: test quality on real data, once I figure out how to do that
+ Refactor: in Utils, say beta = Exp . logBeta, etc, and use those.
- do the todos
+ Possible hack: if instead of a Map I represent a Row as a function
  ColID -> Maybe Double, then I think I can get really good data shape behavior
  + for the output of view_sample and cc_sample to not lose information, this
    rep would need to be traversable

I am somewhat confused about sampling vs prediction.
- I can always define sample as evalState predict (that is, throw
  away the incorporation)
- The laziness might even be smart enough to avoid the extra work
- But, the sample methods I have don't even take a row id, and
  assume it's always a fresh row
- I should also be able to define predict as sample+incorporate
  - May tweak sampling to take the row id and check whether it was
    already present
  - Current incorporate assumes the incorporee is fresh, but could
    make a version that checks

Cleanup:
+ Include Test.hs in the test suite; run the examples with fixed seeds.
+ Rename Test.hs to DPMMTest.hs
+ Do I want to rewrite Assessable a in terms of Model?
+ Do I want to rewrite the two_modes distribution in terms of a
  general fixed-weight-mixture combinator?
+ Rewrite DPMM in terms of the abstractions defined in Models; re-test it
  x Else flush (or delimit?) it; flush DPMM-specific Utils;
+ Convert predictive_logdensity to log domain
- There is a pun in DPMM: I am reusing the data count from GaussStats
  for the CRP occurrence count.
- Do I want to subject DPMM to some Geweke-style testing?
- Do I want to render the DPMM inspection functions as instances of
  the Models classes?
- Measure some kl divergences for Haxcat?
+ Rename the cabal file to haxcat.cabal
+ Get permission from Baxter to relicense as Apache
  + Add copyright headers

Refactorings:
- When gauss_n is zero I get NaN stats :(
- pdf_marginal (and therefore pdf) of NIGNormal may compute niglognorm
  too many times, (4 instead of 2) if GHC doesn't notice that the two
  bases cancel.
- Taylor says "CRP pdf_predictive should perhaps be rewritten in
  terms of bernoulli_weight [or multinomial]".
- Taylor says "it occurred to me that bernoulli_weight ought to have
  an explicit test for zero alpha, in which case it should
  explicitly return -infinity, although I don't know how to express
  that in Haskell."
  - I think Haskell's arithmetic will just do the right thing
- Could add a map of incorporated rows to View (which will be static
  during inference sweeps, but used actively during Geweke testing)
  - Consequence: uninc wouldn't need to accept the Row
  - Funny consequence: inference wouldn't need to accept the dataset
    - To actually do that, would need to reconstruct the column data
      during column sweeps, or keep a redundant view of it
  - May be paying with extra map lookups if I do this

For feature parity:
- Hyperparameter inference
  - Should be able to set up Gibbs sampling by quadrature (maybe the
    Tanh-Sinh package?)
- Conditional predictive simulation and assessment, for known and unknown rows
  - Constrained sampling one row looks a lot like unconstrained, except the
    constraints get passed in and affect the view_cluster_sample distribution
  - Observed sampling is similar, except the view_cluster_sample is a
    delta distribution at the known assignment.
- Handling missing data (presumably by just omitting it from the suff stats)
  instance (Statistic s a) => Statistic (Frob s) Maybe a where ...
  - mumble: different meanings of Maybe: missing observations vs a
    distribution that may choose to produce Nothing.
  - if the homomorphism is a separate class, can extend it to Maybe by
    mapping Nothing to the identity (will GHC flush the redundant
    arithmetic?)
- Handling heterogeneous data
- Permit restriction to columns of interest in the predictive distribution
  (as a work-saving device) (does laziness just do this?)
+ Do I want to mess with dynamic insertion and removal of rows?
  - Multi-row (conditioned) sampling and assessment might benefit from
    this
- multinomial component model
- von Mises component model
- [with baxcat] flesh out component models
- [with baxcat] support uncollapsed component models

For bug hunting:
- Could write a test suite that generates some data and runs
  inference a while, checking that invariants are preserved.
  - quickcheck over the parameters?

For gaining confidence in correctness (of parts):
- Write a 1-D pdf-sampler agreement test and apply it to
  - various components with various stats
  - a mixture with a fixed assignment
- Is there any kind of test that would detect the bug of forgetting to
  incorporate, e.g. in the Geweke transition kernel?
- I ought to unit-test my chi^2 test implementation (possibly to
  assess its strength also)
- Could also do K-S with respect to the Ord instance (would it be
  more sensitive than chi^2?)
  - James Cook wrote a KS test over his random-fu package
    - But only the one-sided version
  - statistics has the necessary pieces
  - people complain that K-S doesn't handle ties well (why not?
    just an implementation artifact, or does the statistic need to
    be adjusted?)
- Bayesian coherence testing (e.g., Geweke):
  - Can Geweke test the NIGNormal code separately, on one instance.
  - Another test for NIG Normal: For all parameter values and all run
    lengths, it induces the same distribution as the uncollapsed
    version.
    - Venture implements this test, and two further variants
  - I really should modify geweke to report the generated data as well
    - And confirm that it changes if I change whether the transition operator incorporates
  - Could run Geweke tests on the per-column cluster counts too
  - Could plot the results of Geweke testing for visual confirmation
- Asymptotic consistency test
  - Generically, can measure approximation of the predictive
    distribution
  - For in-class synthetic data can also measure convergence to the
    latent structure
  - Possible distributions/models to compare:
    - One joint DPMM (with spherical components or a jointly Gaussian component)
    - Per-column DPMMs
    - Just a fat joint Gaussian
    - Single per-column Gaussians with no clustering
- Confirm better predictive accuracy on data synthesized from Crosscat
  than the distractors would get.

Anticipated performance problems:
+ Cache the total in the crp counts object
- Can I replace iterating over map keys and looking things up with
  iterating over the maps?
- the mapZipWith in view_weights traverses the Row, but could be arranged to
  only query (and even win performance if queries were O(1))
- Am I computing column_full_p too many times?
- Will it be important to store the data table in a flat 2D form
  rather than Map ColID (Vector Double)?
- Is it possible that the Loom hack that vkm is so proud of was simply
  to reshuffle the data store and reassign row and column ids after
  every gibbs sweep to keep them contiguous?
  - If so, could replace maps with flat arrays in the metadata too,
    for the compute segments when they do not change
  - Shuffling columns is likely to be difficult to reconcile with
    making the representation strongly typed
    - An existential that holds a proxy describing the layout might work

Performance notes
- Benchmark results
  - train (bogodata 1167 23) 1500 takes about 10 minutes
    - around 3x faster than crosscat
  - train (bogodata2 1167 23) 15 takes about 1 minute
    - around 3x slower than crosscat (but tons of clusters -- too many?)
  - crosscat suffers from multinomials in satellites, so perhaps this
    comparison is a little unfair.
  - flushing empty clusters seemed to help, but across-seed
    variability is high
- Profile instructions:
  - cabal configure --enable-executable-profiling
  - cabal build bench-infer && time dist/build/bench-infer/bench-infer +RTS -p -hc
- Likely hotspots:
    view_weights.likelihood.likelihood_one  RowSweep  18.3   20.2
    pdf_marginal.niglognorm                 Models    13.5    3.4
    pdf_marginal                            Models     9.5    1.7
- Persistent memory usage is good: 400k
  - But, around 100k of it is boxed doubles?
  - another around 100k is interior nodes of Data.Maps
- Probably some numeric code didn't get unboxed properly
- Strictifying (and presumably therefore unboxing) the fields of
  GaussStats and NIGNormal reduces allocation by about a third,
  doesn't seem to help overall speed, but seems to shift work into
  the numerical computations
- Explicitly unboxing the vectors that hold the data set helps
  persistent memory use some, and drastically reduces the number D#
  constructors being retained

Status of testing DPMM: I have a 4-parameter program that computes
estimates of KL divergence.
- The KL computation looks pretty plausible
  - could be tried on more distributions, e.g. nested Gaussians
- Anecdotal runs produce believable results; is there a way to
  systematize?
  - It seems like I want to automate exploration along at least some
    dimensions, so I can do a run that actually validates asymptotic
    certainty on my example.

Thoughts about invariants:
- The non-cache information in a Crosscat is
  - The top crp alpha
  - The cc_partition
  - For each view
    - The view crp alpha
    - The view partition
  - For each column
    - The column hypers
- The major invariant: Rebuilding from the data with the given
  view partition and cluster partitions produces the same result
  up to floating point in the suff stats.  This implies:
  - All floats are finite
  - Column stats agree with the dataset and the partition up to
    floating point
  - The currently tested structural invariants
    - View counts agree with the partition (and count total agrees)
      - No zero values in view counts
    - cc counts agree with the partition (and count total agrees)
      - No zero values in cc counts
    - The set of cluster ids in a column is the set of cluster ids
      in the partition
    - Ditto keys of cc_views vs values of cc_partition
    - A ColID is assigned to a view iff it assigns that ColID to a
      Column: set of a value's keys in cc_partition equals set of
      keys in that view's columns map
    - The views are rectangular in the RowIDs
- Major dynamic invariant: None of the transformations lose (or
  synthesize) row or column ids

References (from Baxter):
- Gaussians (including multivariate):
  http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf
  - He parameterizes his non-standard t-distributions by location and
    squared scale.
- Other things (don't trust his math for the von-Mises):
  http://www.leg.ufpr.br/lib/exe/fetch.php/projetos:mci:tabelasprioris.pdf
- Crosscat as implemented doesn't have multivariate component models,
  just products of univariates.  The only thing they share is the
  partition.
  - Consequently don't need multi-dimensional normal-inverse-wishart
- According to Baxter, the component model for categorical data is a
  multinomial with a symmetric Dirichlet prior.
