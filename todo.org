Things to do (+ bullet means "done"):
+ Implement the class hierarchy refactoring that Taylor suggested
- Write a 1-D pdf-sampler agreement test and apply it to
  - various components with various stats
  - a mixture with a fixed assignment
+ See whether Taylor's View proposal leads to a Crosscat type I believe
+ Fill in easy undefined bits for a full col_step
+ Fill in sampling a view from the view prior 
+ Draft a full column Gibbs sweep
+ Draft row transitions
+ Draft a full row Gibbs sweep
+ Benefit: compile full inference except for hyperparameters
+ Draft initialization against a dataset of the desired type
+ Benefit: Smoke test running the program
  - To get stack traces
    cabal configure --enable-executable-profiling
    cabal build && dist/build/test-smoke/test-smoke +RTS -xc
+ Benefit: benchmark inference vs crosscat on ridiculous data
  - train (bogodata 1167 23) 1500 takes about 10 minutes
    - around 3x faster than crosscat
  - train (bogodata2 1167 23) 15 takes about 1 minute
    - around 3x slower than crosscat (but tons of clusters -- too many?)
  - crosscat suffers from multinomials in satellites, so perhaps this
    comparison is a little unfair.
  - flushing empty clusters seemed to help, but across-seed
    variability is high
- "Crash" testing:
  + Fix the random seed in HaxcatTest and compare to known string
  + Convert TestSmoke.hs into a cabalized benchmark executable
  + Write code checking structural invariants
  - could write some diagnostic code, like # views, # clusters,
    and maybe watch it evolve
  - could write a test suite that generates some data and runs
    inference a while, checking that invariants are preserved.
    - quickcheck over the parameters?
  + Exercise prediction and assessment (e.g., it executes)
- "Crash" fixes:
  - when gauss_n is zero I get NaN stats :(
  + Flush empty clusters
- Refactorings:
  + Flush NoStat?
  + Can I get rid of zipWithHoles via mergeWithKey?
  + Define more compact Show instances for newtypes of ints: Ro, Co, Cl, Vw
  - Taylor says "CRP pdf_predictive should perhaps be rewritten in
    terms of bernoulli_weight [or multinomial]".
  - Taylor says "it occurred to me that bernoulli_weight ought to have
    an explicit test for zero alpha, in which case it should
    explicitly return -infinity, although I don't know how to express
    that in Haskell."
    - I think Haskell's arithmetic will just do the right thing
  - Taylor says "For NIGNormal pdf, I think you can write this: pdf m
    x = pdf_predictive empty m x.  In fact, that should just be called
    compound_pdf"
  - Could migrate the Map ColID Column out to the Crosscat object.
    Then each View is more clearly just the set of applications of a
    crp (mapping rows to clusters); and I may be able to reuse that
    object in the relevant part of crosscat too.
- Benefit: profile inference
  cabal configure --enable-executable-profiling
  cabal build bench-infer && time dist/build/bench-infer/bench-infer +RTS -p -hc
- Benefit: geweke test 1 x m
  - Only tests the NIG Normal code
  - geweke runner ought to have type
    RVar a -> (a -> RVar b) -> (b -> a -> RVar a) -> (a -> b -> Probe) -> Results
    except somebody needs to deal with unincorporating and
    reincorporating any collapsed models in a that depend on the data b
    - Can I make the transition operator do that?
    - Can I get a version of Crosscat uninc that will conserve the
      partitions while removing the data, for simulating new data
      conditioned on the partition?
    - This is sensible, because the cluster stats are the only things
      in the thing that depend on the content rather than the shape of
      the data.
    - Possible kludge: define an "all-empty simulate" that only reads
      the partitions and hyper parameters (and any uncollapsed cluster
      parameters, when that starts happening)
- Benefit: asymptotic consistency test
  - Generically, can measure approximation of the predictive
    distribution
  - For in-class synthetic data can also measure convergence to the
    latent structure
+ Draft predictive simulation
+ Draft predictive pdf
- Draft multi-row joint simulation
- Benefit: geweke test n x m
- Is there a notion of asymptotic consistency with a joint multi-row
  generator?
  - Are such things ever in the hypothesis space?
- Benefit: benchmark inference vs crosscat on synthetic data n x m
- Draft collecting a data table
- Benefit: benchmark inference vs crosscat on real data
- Benefit: benchmark simulation vs crosscat on real data
- Benefit: test quality on real data, once I figure out how to do that
+ Refactor: in Utils, say beta = Exp . logBeta, etc, and use those.
- do the todos
+ Possible hack: if instead of a Map I represent a Row as a function
  ColID -> Maybe Double, then I think I can get really good data shape behavior
  - for the output of view_sample and cc_sample to not lose information, this
    rep would need to be traversable
  - the mapZipWith in view_weights traverses the Row, but could be arranged to
    only query (and even win performance if queries were O(1))

Cleanup:
- Rewrite DPMM in terms of the abstractions defined in Models; re-test it
  - Else flush (or delimit?) it; flush DPMM-specific Utils;
- Generalize Test.hs to also test Haxcat
+ Rename the cabal file to haxcat.cabal
+ Get permission from Baxter to relicense as Apache
  + Add copyright headers

For feature parity:
- Hyperparameter inference
- Conditional predictive simulation and assessment, for known and unknown rows
  - Constrained sampling one row looks a lot like unconstrained, except the
    constraints get passed in and affect the view_cluster_sample distribution
  - Observed sampling is similar, except the view_cluster_sample is a
    delta distribution at the known assignment.
- Handling missing data (presumably by just omitting it from the suff stats)
  instance (Statistic s a) => Statistic (Frob s) Maybe a where ...
  - mumble: different meanings of Maybe: missing observations vs a
    distribution that may choose to produce Nothing.
  - if the homomorphism is a separate class, can extend it to Maybe by
    mapping Nothing to the identity (will GHC flush the redundant
    arithmetic?)
- Handling heterogeneous data
- Permit restriction to columns of interest in the predictive distribution
  (as a work-saving device)
- Do I want to mess with dynamic insertion and removal of rows?
  - Multi-row (conditioned) sampling and assessment might benefit from
    this
- multinomial component model
- von Mises component model
- [with baxcat] flesh out component models
- [with baxcat] support uncollapsed component models

Future performance problems:
+ Cache the total in the crp counts object
- Can I replace iterating over map keys and looking things up with
  iterating over the maps?
- Am I computing column_full_p too many times?
- Will it be important to store the data table in a flat 2D form
  rather than Map ColID (Vector Double)?
- Is it possible that the Loom hack that vkm is so proud of was simply
  to reshuffle the data store and reassign row and column ids after
  every gibbs sweep to keep them contiguous?
  - If so, could replace maps with flat arrays in the metadata too,
    for the compute segments when they do not change
  - Shuffling columns is likely to be difficult to reconcile with
    making the representation strongly typed
    - An existential that holds a proxy describing the layout might work

Status of testing DPMM: I have a 4-parameter program that computes
estimates of KL divergence.
- The KL computation looks pretty plausible
  - could be tried on more distributions, e.g. nested Gaussians
- Anecdotal runs produce believable results; is there a way to
  systematize?
  - It seems like I want to automate exploration along at least some
    dimensions, so I can do a run that actually validates asymptotic
    certainty on my example.

Thoughts about invariants:
- The non-cache information in a Crosscat is
  - The top crp alpha
  - The cc_partition
  - For each view
    - The view crp alpha
    - The view partition
  - For each column
    - The column hypers
- The major invariant: Rebuilding from the data with the given
  view partition and cluster partitions produces the same result
  up to floating point in the suff stats.  This implies:
  - All floats are finite
  - Column stats agree with the dataset and the partition up to
    floating point
  - The currently tested structural invariants
    - View counts agree with the partition (and count total agrees)
      - No zero values in view counts
    - cc counts agree with the partition (and count total agrees)
      - No zero values in cc counts
    - The set of cluster ids in a column is the set of cluster ids
      in the partition
    - Ditto keys of cc_views vs values of cc_partition
    - A ColID is assigned to a view iff it assigns that ColID to a
      Column: set of a value's keys in cc_partition equals set of
      keys in that view's columns map
    - The views are rectangular in the RowIDs
- Major dynamic invariant: None of the transformations lose (or
  synthesize) row or column ids

References (from Baxter):
- Gaussians (including multivariate):
  http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf
  - He parameterizes his non-standard t-distributions by location and
    squared scale.
- Other things (don't trust his math for the von-Mises):
  http://www.leg.ufpr.br/lib/exe/fetch.php/projetos:mci:tabelasprioris.pdf
- Crosscat as implemented doesn't have multivariate component models,
  just products of univariates.  The only thing they share is the
  partition.
  - Consequently don't need multi-dimensional normal-inverse-wishart
- According to Baxter, the component model for categorical data is a
  multinomial with a symmetric Dirichlet prior.
